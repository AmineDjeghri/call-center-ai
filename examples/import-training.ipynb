{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-documentintelligence==1.0.0b1 azure-search-documents==11.4.0 unidecode==1.3.8 nltk==3.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus, install NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "\n",
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize the clients to Document Intelligence and AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence.aio import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.search.documents import SearchClient\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import randint\n",
    "from typing import Dict, Tuple\n",
    "from unidecode import unidecode\n",
    "import asyncio\n",
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "doc_endpoint = \"https://claim-ai.cognitiveservices.azure.com\"\n",
    "doc_credential = AzureKeyCredential(\"xxx\")\n",
    "doc_client = DocumentIntelligenceClient(\n",
    "  endpoint=doc_endpoint,\n",
    "  credential=doc_credential,\n",
    ")\n",
    "\n",
    "search_endpoint = \"https://claim-ai.search.windows.net\"\n",
    "search_credential = AzureKeyCredential(\"xxx\")\n",
    "search_client = SearchClient(\n",
    "  endpoint=search_endpoint,\n",
    "  index_name=\"trainings\",\n",
    "  credential=search_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transform PDF in Markdown text. We are using Document Intelligence for that.\n",
    "\n",
    "Be warned that this step can take a few minutes, depending on the size of the PDF. From a minute for a few pages to 20 minutes for a 1000 pages PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pdf_to_markdown(source: str) -> Tuple[str, str]:\n",
    "    if os.path.exists(source + \".md\"):  # Test cache\n",
    "        print(f\"Skipping {source}, cached found\")\n",
    "        with open(source + \".md\", \"r\") as file:\n",
    "            return source, file.read()\n",
    "\n",
    "    with open(source, \"rb\") as file:  # Load file content\n",
    "        print(f\"Starting {source}, no cache found\")\n",
    "        await asyncio.sleep(randint(0, 5))  # Avoid API rate limit\n",
    "        doc_poller = await doc_client.begin_analyze_document(\n",
    "            analyze_request=file,\n",
    "            content_type=\"application/octet-stream\",\n",
    "            locale=\"fr-FR\",  # We only have French documents in this dataset\n",
    "            model_id=\"prebuilt-layout\",\n",
    "            output_content_format=\"markdown\",\n",
    "        )\n",
    "        doc_result = await doc_poller.result()\n",
    "\n",
    "        with open(source + \".md\", \"w\") as file:  # Store result in cache\n",
    "            file.write(doc_result.content)\n",
    "\n",
    "        return source, doc_result.content\n",
    "\n",
    "\n",
    "doc_results: Dict[str, str] = {}\n",
    "doc_tasks = []\n",
    "\n",
    "for source in glob.glob(\"dataset/*.pdf\"):\n",
    "    doc_tasks.append(asyncio.create_task(pdf_to_markdown(source)))\n",
    "\n",
    "print(\"Waiting for results...\")\n",
    "for doc_task in asyncio.as_completed(doc_tasks):\n",
    "    source = None\n",
    "    try:\n",
    "        source, content = await doc_task\n",
    "        print(f\"Ended {source}\")\n",
    "        doc_results[source] = content\n",
    "    except HttpResponseError as e:\n",
    "        print(f\"Failed {source}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Markdown text into smaller blocks, we'll call chuncks. Each block content is minified with a stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"french\"\n",
    "stemmer = SnowballStemmer(lang)\n",
    "\n",
    "\n",
    "def compress_and_clean(text: str) -> str:\n",
    "    text = text.replace(\"\\\\\", \"\")  # Remove all backslashes\n",
    "    text = re.sub(r\":[a-z]*:\", \"\", text)  # Remove all :unselected: and :selected: tags\n",
    "    text = re.sub(r\"<!--[^<>]*-->\", \"\", text)  # Remove all comments\n",
    "    tokenized = word_tokenize(text, language=lang)\n",
    "    tokens = [stemmer.stem(token) for token in tokenized]\n",
    "    prompt = \" \".join(tokens)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def data(content: str, source_uri: str, title: str, iterator: int) -> dict[str, str]:\n",
    "    return {\n",
    "        \"content\": compress_and_clean(content),\n",
    "        \"id\": f\"{'_'.join(re.sub('[^a-z0-9]', ' ', unidecode(source_uri).lower()).split())}-{iterator}\",  # Use deterministic ID to avoid duplicates after a new run\n",
    "        \"source_uri\": unidecode(source_uri).lower(),\n",
    "        \"title\": ' '.join(re.sub('[^a-z0-9]', ' ', unidecode(title).lower()).split()),  # Remove all special characters\n",
    "    }\n",
    "\n",
    "\n",
    "chuncks = []\n",
    "iterator = 0\n",
    "\n",
    "for source, content in doc_results.items():\n",
    "  if not content:\n",
    "    continue\n",
    "  for section in content.split(\"\\n#\"):\n",
    "    lines = section.split(\"\\n\")\n",
    "    title = lines[0].strip()\n",
    "    paragraph = \" \".join(lines[1:])\n",
    "    if not section:\n",
    "      continue\n",
    "    chuncks.append(data(\n",
    "      f\"# {title} {paragraph}\",\n",
    "      source,\n",
    "      title,\n",
    "      iterator,\n",
    "    ))\n",
    "    iterator += 1\n",
    "\n",
    "print(chuncks[:5])\n",
    "print(f\"Created {len(chuncks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upload the chuncks to AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Uploading {len(chuncks)} documents to Azure Search\")\n",
    "search_client.merge_or_upload_documents(chuncks)\n",
    "print(f\"There are {search_client.get_document_count()} documents in the index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! ðŸ˜Ž**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Clean up the documents from the AI Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    docs = search_client.search(search_text=\"*\", select=[\"id\"])\n",
    "    ids = [{\"id\": doc[\"id\"]} for doc in docs if doc[\"id\"]]\n",
    "    if not ids:\n",
    "        break\n",
    "    print(ids[:5])\n",
    "    search_client.delete_documents(ids)\n",
    "    print(f\"Deleted {len(ids)} documents\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
